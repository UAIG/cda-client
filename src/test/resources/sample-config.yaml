sourceLocation:
  bucketName: dummy-bucket
  manifestKey: dummy/key.json
outputLocation:
  path: dummy/path
savepointsLocation:
  uri: file:///dummy/path
outputSettings:
  exportTarget: file
  saveIntoJdbcRaw: true
  saveIntoJdbcMerged: false
  fileFormat: csv
  includeColumnNames: true
  saveAsSingleFile: true
  #saveIntoTimestampDirectory: true #Should default missing boolean to false
jdbcConnectionRaw:
###POSTGRESQL###
  jdbcUsername: postgres
  jdbcPassword: Gw_123!
  jdbcUrl: 'jdbc:postgresql://localhost/CDARaw'
  jdbcSchema: public
  jdbcSaveMode: append
jdbcConnectionMerged:
###POSTGRESQL###
  jdbcUsername: postgres
  jdbcPassword: Gw_123!
  jdbcUrl: 'jdbc:postgresql://localhost/CDA'
  jdbcSchema: public
  dataTimeZone: UTC
# This section is optional
# performanceTuning:
#   numberOfJobsInParallelMaxCount: 5
#   numberOfThreadsPerJob: 10
metricsSettings:
  batchMetricsValidationEnabled: true
  ignoreBatchMetricsErrors: true
  updateMismatchWarningsEnabled: true
  logUnaffectedUpdates: true
  sinkSettings:
    '*.sink.console.class': 'org.apache.spark.metrics.sink.ConsoleSink'
sparkSettings:
  'spark.ui.port': 4040
connectionPoolSettings:
  maximumPoolSize: 100
